---
layout: post
comments: false
title:  "Chapter 1: All Things dataset..."
excerpt: "Dataset is at the heart of every Machine Learning task. In this post, I've explored the Shakespeare Dataset and explained concepts related to tokenization, Token Embedding and Positional Encoding"
date:   2024-04-28 10:00:00

---

## Introduction

Whenever someone talks about Transformer now a days, more often than not, they mean Generative models like GPT. However, not a lot of people know that originally Transformers were designed for Machine Translation. A user would input a sentence in one language and the model would translate that to another. This was done using an encoder-decoder styled architecture, where the encoder accepts the input and understands the context in this particular sentence. This information is then passed over to the decoder which starts generating the translation. 

In this work, I'll be focusing on the task of Machine Translation using the "Shakespeare" dataset. Starting with:

- The dataset (which contains the excerpts from Shakespeare and their meaning), how it is tokenized and preprocessed for training the Neural Net.

- I will then spend some time discussing the token embeddings and positional encodings. The special thing about these embeddings is that they will be learned alongside the network parameters.

- After that, I'll move on to the Transformer Neural Net, where

    - I'll delve into the details that make up the encoder, specifically I'll focus more on the Multi-head Self Attention and how it learns the relationship between different words. The hope here is that the encoder can understand the context in the input sentence, because

    - This will guide the decoder (when prompted), to start translating the sentence. 

- I will also discuss the training procedure in detail and more specifically how the loss function enables the generator to make accurate predictions.

## Dataset

For training the Neural Net, I'm working with [shakespearefy](https://www.kaggle.com/datasets/garnavaurha/shakespearify) dataset. One thing that I'm doing different (or atleast something that's not a standard practice) is that I'm not splitting the dataset into training and testing subsets. I have two reasons as to why I'm doing this:

- It's a really small dataset of size ~50k

- My goal is to just understand how an autoregressive transformer works.

After downloading the dataset, you'll notice that it is in a simple csv format. Loading and visualizing it is quite easy.

```py
# Loading the csv file
df = pd.read_csv("../data/shk2mod.csv", index_col=0)                            
# Dropping the index column                                 
df.drop("id", axis=1, inplace=True)                  
# Renaming columns                        
df.rename(columns={"og": "shakespeare eng", "t": "modern eng"}, inplace=True)   

# Visualizing Sentences
print(df)
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>shakespeare eng</th>
      <th>modern eng</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>You do not meet a man but frowns:</td>
      <td>Every man you meet these days is frowning.</td>
    </tr>
    <tr>
      <th>1</th>
      <td>our bloods  No more obey the heavens than our...</td>
      <td>Our bodies are in agreement with the planetar...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>But what's the matter?</td>
      <td>What's wrong?</td>
    </tr>
    <tr>
      <th>3</th>
      <td>His daughter, and the heir of's kingdom, whom...</td>
      <td>The king wanted his daughter, the only heir to...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>she's wedded;  Her husband banish'd; she impr...</td>
      <td>She's married, her husband is banished, she's...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>51782</th>
      <td>He hath not told us of the captain yet.</td>
      <td>He hasn't told us about that captain yet.</td>
    </tr>
    <tr>
      <th>51783</th>
      <td>When that is known and golden time convents, ...</td>
      <td>When that's taken care of and the time is conv...</td>
    </tr>
    <tr>
      <th>51784</th>
      <td>Meantime, sweet sister,  We will not part fro...</td>
      <td>Until then, sweet sister-in-law, we won't leav...</td>
    </tr>
    <tr>
      <th>51785</th>
      <td>Cesario, come,  For so you shall be, while yo...</td>
      <td>Cesario, come here. You'll be Cesario to me wh...</td>
    </tr>
    <tr>
      <th>51786</th>
      <td>When that I was and a little tiny boy,    With...</td>
      <td>When I was just a tiny little boy,With hey, ho...</td>
    </tr>
  </tbody>
</table>
<p>51787 rows × 2 columns</p>
</div>