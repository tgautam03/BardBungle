---
layout: post
comments: false
title:  "Chapter 2: Encoder"
excerpt: "Let's understand the relationship between different words in the Shakespeare sentences."
date:   2024-05-22 20:00:00

---

## Recap and Objectives
In the last chapter, I discussed the dataset. Starting from the raw strings, we saw how it is preprocessed into a sequence of integer tokens. These tokens are then fed into token embedding where each integer is mapped to a high dimensional vector (with some sense of language semantics). Finally, positional encoding adds the information related to the position of each token in the sequence.

In this chapter, I will spend some time discussing self attention. Starting from the very basic intuition of the operations behind self attention, I will build an Encoder network which processes the input shakespeare sentences and understand the context between different tokens.

## Introduction
After token embedding and positional encoding, each token is expressed as a high dimensional vector with some global context. A way to think about this is that if I take the vector representation of king, subtracts the vector represetnation of boy and add the vector representation of of girl, the result should be close to the vector representation of queen. All this is important but it has nothing to do with individual sentences. In the sentence "You do not meet a man but frowns:", there is a relationship between word "meet" and "you", "man" and "frown" which cannot be encoded in the global context. This is where attention mechanism comes in.


## What is Attention
The purpose of Attention is to assign a "similarity score" between tokens in sequences. Let me explain this using a very simple example. Consider a sequence containing 5 different movies. Each movie has 3 attributes (comedy, adventure and animated) which can take values from -1 to 1 (see Figure 1). For instance, movie 1 is pure comedy so it's attributes would be something like [1, -1, -1]. However, another movie is animated with adventure and a bit of comedy. The attribute values in this case would look something like [0.2, 0.8, 1]. Similarly we have attribute values assigned to rest of the movies. The task here is to recommend movies to 3 different users with different movie preferences. Similar to the movies, these users can also be put into a sequence with attribute values representing their movie preferences (see Figure 2).

<div class="imgcap">
<img src="https://raw.githubusercontent.com/tgautam03/BardBungle/gh-pages/assets/ch2/movies.png" alt="this slowpoke moves"  width="500"/>
<div class="thecap">Figure 1: Vector representation of movies. </div>
</div>

<div class="imgcap">
<img src="https://raw.githubusercontent.com/tgautam03/BardBungle/gh-pages/assets/ch2/users.png" alt="this slowpoke moves"  width="500"/>
<div class="thecap">Figure 2: Vector representation of users. </div>
</div>

The next question is, how can we mathematically evaluate which user will like which movie more? The answer is dot product!

If we premultiply the users matrix with the transpose of the movies matrix (i.e. match the attribute values in users against movies), we will get a score telling us how much a given user might like a movie.

```py
# Dot product between Users and Movie
score = users @ movies.T
```
This score could be any floating point number. It could end up being a very large number if there are more attribute values, so it's better to scale this by the square root of number of attributes (3 in this case).

> Why $\sqrt{3}$? Its the Euclidean length of the vectors in the sequence. Therefore, we are dividing out the amount by which the increase in dimension increases the length of the average vectors.

```py
# Scaling down the score
score = score / (3**(1/2))
```
Now the score is scaled nicely and won't blow up as the number of attributes increase, but it would still be better to have this number between 0 and 1. This is done by applying row wise softmax to the score matrix.

> Why row wise softmax? We want to analyse how a user would like different movies, hence scores across a set user need to sum up to 1.

```py
# Row wise Softmax
score = F.softmax(score, dim=-1)
```
<div class="imgcap">
<img src="https://raw.githubusercontent.com/tgautam03/BardBungle/gh-pages/assets/ch2/dot.png" alt="this slowpoke moves"  width="500"/>
<div class="thecap">Figure 3: Attention weights for each user. </div>
</div>

This concept can be applied straight to the Shakespeare dataset. Instead of the sequence of users, we just have a sequence of tokens in a sentence. The task for the Transformer is to understand the context between different tokens in this sentence, so the same sequence of tokens replace the sequence of movies as well.

> Remember that the attributes of a token in this case are generated by the Token Embedding and Positional Encoding layers.

Figure 4 shows the attention appied to a shakespeare sentence with the following details:
```
-------------------Shakespeare Dataset-------------------------
Original Sentence:   You do not meet a man but frowns: 
Tokenized Sentence:  ['You', 'do', 'not', 'meet', 'a', 'man', 'but', 'frown', 's', ':', '[EOS]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']
Numericalized Sentence:  tensor([ 107,   48,   23,  427,   12,   87,   50, 1530,    6,   26,    2,    0,
           0,    0,    0])
Attention Weights for the sentence: 
```
<div class="imgcap">
<img src="https://raw.githubusercontent.com/tgautam03/BardBungle/gh-pages/assets/ch2/sk_attn.png" alt="this slowpoke moves"  width="800"/>
<div class="thecap">Figure 4: Attention weights for a shakespeare sentence. </div>
</div>

Now to embed this contextual information, weight matrix `W` is multiplied with the original sequence matrix.
```py
# Embedding the contextual information to generate the modified sequence
y = torch.matmul(W, data[0])
```

You might have noticed a problem with the weight matrix. Each token is only attending to itself! That's not very useful. We need a mechanism that learns the context automatically based on the provided dataset. This is where we incorporate machine learning into the self attention operation.

## Adding Machine Learning to Self Attention
Instead of using the sequence of tokens `x` as it is, we send it through three different neural nets (with learnable parameters) and collect the outputs. Let's call these `queries`, `keys` and `values`.

```py
queries = torch.nn.Linear(emb, emb)(x)
keys = torch.nn.Linear(emb, emb)(x)
values = torch.nn.Linear(emb, emb)(x)
```

The outputs are then used to get attention weights, which in turn modifies the input sequence (`values`) such that it contains the context between different tokens.

```py
W = torch.matmul(queries, keys.transpose(-2,-1)) # Computing Weights
W = W / (self.emb**(1/2)) # Scaling for stability
W = F.softmax(W, dim=-1) # Row-wise Softmax
y = torch.matmul(W, values) # Computing y
```